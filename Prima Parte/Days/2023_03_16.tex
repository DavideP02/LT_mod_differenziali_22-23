\days{16 marzo 2023}

\paragrafo{Ripasso}{%
    Un \emph{sistema lineare di equazioni differenziali ordinarie} è un sistema della forma \[
        \bm{x}'=A(t)\bm{x}+\bm{b}(t)
    \]dove \[
        \bm{x}=\bm{x}(t)=\begin{bmatrix}
            x_1(t)\\ 
            \vdots\\ 
            x_{n}(t) 
        \end{bmatrix}, \quad \bm{b}(t)=\begin{bmatrix}
            b_1(t)\\ 
            \vdots\\ 
            b_{n}(t) 
        \end{bmatrix},\quad A(t)=\begin{bmatrix}
            a_{11}(t) & \dots & a_{1n}(t)\\ 
            \vdots & \ddots & \vdots\\ 
            a_{n1}(t)  & \dots & a_{nn}(t) 
        \end{bmatrix}.
    \]e tutte le funzioni sono continue su un intervallo aperto $ I \subseteq \R $. 
    
    Se $ b_1,\dots,b_{n}  $ sono tutte nulle, il sistema si dice \emph{omogeneo}.
}{}{}
\paragrafo{Caso monodimensionale}{%
    Sia $ f(t,x)=A(t)\,x + b(t) $, $ f \in C^{1}(I\times \R) $. Per ogni condizione iniziale $ (t_0,x_0) \in I\times \R $, il problema di Cauchy associato al sistema ammette un'unica soluzione locale. 

    Essendo \[
        \pd{f}{x}(t,x)=A(t)
    \]continua su $ I $, abbiamo che su ogni striscia $ [a,b]\times \R \subseteq I\times \R$ tale funzione è limitata. Questo ci garantisce che ogni problema di Cauchy associato al sistema ammette un'unica soluzione definita sull'intero intervallo $ I $.
}{}{}
\paragrafo{Alcuni Risultati}{%
    Posto \[
        S_{\bm{b}}\coloneqq \{\text{soluzioni di } \bm{x}'=A(t)\bm{x}+\bm{b}(t)\} 
    \]valgono i seguenti risultati. \begin{itemize}
        \item \emph{Principio di Sovrapposizione}. Se $ \bm{x}_1 \in S_{\bm{b}_1}  $ e $ \bm{x}_2 \in S_{\bm{b}_2} $, allora \[
            \bm{x}_1+\bm{x}_2 \in S_{\bm{b}_1+\bm{b}_2} 
        \]
        \item \emph{Soluzioni di un sistema omogeneo}. $ S_{\bm{0}}  $ è isomorfo a $ \R^{n} $. 
        \item \emph{Soluzioni di un sistema non omogeneo}. Se $ \bm{x}_P $ risolve $ \bm{x}'=A(t)\,\bm{x}+\bm{b}(t) $, allora \[
            S_{\bm{b}}=\{\bm{x}_0+\bm{x}_P: \bm{x}_0 \in S_{\bm{0}} \} 
        \]
        \item \emph{Lemma}. Siano $ \bm{y}_1,\dots,\bm{y}_n $ delle $ n $ soluzioni del problema omogeneo $ \bm{x}'=A(t)\,\bm{x} $. Allora $ \bm{y}_1,\dots,\bm{y}_n $ sono funzioni linearmente indipendenti se e solo se $\exists\, t_0 \in I$ tale che i vettori \[
            \bm{y}_1(t_0),\dots,\bm{y}_n(t_0)
        \]siano linearmente indipendenti in $ \R^{n} $.
    \end{itemize}
}{}{}
\paragrafo{Matrice Wronskiana}{%
    Se $ \bm{\varphi}_1,\dots,\bm{\varphi}_n $ sono $ n $ soluzioni linearmente indipendenti del sistema omogeneo $ \bm{x}'=A(t) \, \bm{x}$, allora $ \{\bm{\varphi}_1,\dots,\bm{\varphi}_n\} $ si dice \emph{insieme fondamentale}. 

    La matrice \[
        W(t)=\begin{bmatrix}
            \bm{\varphi}_1 & \dots & \bm{\varphi}_n
        \end{bmatrix}
    \]si dice \emph{matrice wronskiana}.

    Avendo che $ \displaystyle S_{\bm{0}} = \{W(t)\,\bm{c}: \bm{c} \in \R^{n}\} $, cerchiamo come selezionare in $ S_{\bm{0}}  $ la soluzione di \[
        \begin{cases}
            \bm{x}'=A(t)\,\bm{x}\\ 
            \bm{x}(t_0)=\bm{x}_0
        \end{cases}
    \]Imponiamo $ W(t_0)\,\bm{c} = \bm{x}_0 $. Essendo le colonne di $ W(t_0) $ linearmente indipendenti, $ W(t_0) $ è invertibile e $ \bm{c}=\left[W(t_0)\right]^{-1}\,\bm{x}_0 $.\\ La soluzione dunque è \[
        \bm{x}(t)=W(t)\,\left[W(t_0)\right]^{-1}\,\bm{x}_0
    \]
}{}{}
\paragrafo{Matrice risolvente}{%
    Se $ W $ è una matrice wronskiana e se, per qualche $ t_0 \in I$, si ha $ W(t_0)=\id_{n}  $, allora $ W $ viene detta \emph{matrice risolvente} o \emph{di transizione} o \emph{di monodromia}. 

    Se $ W(t) $ è una matrice wronskiana per \[
        \begin{cases}
            \bm{x}'=A(t)\,\bm{x}\\ 
            \bm{x}(t_0)=\bm{x}_0
        \end{cases}
    \]allora \[
        \Phi(t)=W(t)\,\left[W(t_0)\right]^{-1}
    \]è di monodromia.
}{}{}
\paragrafo{Equazioni differenziali lineari di grado $ n $}{%
    Consideriamo l'equazione differenziale lineare \[
        y^{(n)}(t)=a_{n-1}(t)\,y^{(n-1)}(t)+ a_{n-2}(t)\,y^{(n-2)}(t) + \dots+ a_0(t)\,y(t) + b(t).
    \]Supponendo che tutte le $ a_{i}  $ e $ b $ siano continue su $ I \subseteq \R $ intervallo. Definiamo \[
        x_1(t)=y(t), \quad x_2(t)=y'(t), \quad\dots,\quad x_{n}(t)=y^{(n-1)}(t) 
    \]
    Otteniamo così il sistema lineare del primo ordine $ \bm{x}'=A(t)\,\bm{x}+\bm{B}(t) $ con \[
        A(t)=\begin{bmatrix}
            0 & 1 & 0 & \dots & 0\\ 
            0 & 0 & 1 & \dots & 0\\ 
            \vdots\\ 
            0 & & & & 1\\ 
            a_0(t) & a_1(t) & \dots & a_{n-1}(t) &0
        \end{bmatrix},\quad \bm{B}(t)=\begin{bmatrix}
            0\\ 
            0\\ 
            \vdots\\ 
            0\\ 
            b(t)
        \end{bmatrix}
    \]
}{}{}
\paragrafo{Obiettivo}{%
    L'obiettivo è risolvere l'equazione \[
        \bm{x}'=A\,\bm{x},\qquad A \in \R^{n,n}, \bm{x} \in \R^{n}
    \]L'esistenza delle soluzioni è garantita su tutto $ \R $. 

    Analizzeremo diverse casistiche:\begin{enumerate}
        \item $ A $ diagonale;
        \item $ A $ diagonalizzabile;
        \item $ A $ con autovalori in $ \C $, tutti autovalori regolari\footnote{Un autovalore è regolare se la molteplicità algebrica e geometrica coincidono};
        \item caso generale.
    \end{enumerate}
}{}{}

\osservazione{
    Per un'equazione della forma $ \bm{x}'=A\,\bm{x}$, $A \in \R^{n,n} $, $ \bm{x}=\bm{0} $ è sempre soluzione (e quindi è equilibrio).

    E' facile osservare, in realtà, che gli equilibri del sistema sono proprio gli elementi del $ \ker A $.
}

\section{Matrice diagonale}

\paragrafo{Risoluzione generica}{%
Il caso in cui $A$ è una matrice diagonale è abbastanza semplice. Consideriamo il sistema $ \bm{x}'=A\,\bm{x} $, con \[
        A=\begin{pmatrix}
            \lambda_1\\ 
            & \lambda_2\\ 
            & & \lambda_3\\ 
            & & & \ddots\\ 
            & & & & \lambda_{n} 
        \end{pmatrix}\in \R^{n,n}
    \]
Questo è equivalente a: \[
        \begin{cases}
            x_1'(t)=\lambda_1\,x_1(t) \\
            x_2'(t)=\lambda_2\,x_2(t) \\
            \vdots
        \end{cases}
    \] 
    
    Integrando otteniamo che $ \displaystyle x_{i}(t) = c_{i}\,e^{\lambda_{i}\,t }   $ per ogni $ i=1,\dots, n $, dove $ c_{i}  $ è una costante arbitraria. 
    Abbiamo così costruito la nostra soluzione:
    
    $\implies$ $ \displaystyle \bm{x}(t) = \parentesi{W(t)}{\begin{pNiceMatrix}
        e^{t\,\lambda_1}\\ 
            & e^{t\,\lambda_2}\\ 
            & & e^{t\,\lambda 3}\\ 
            & & & \Ddots\\ 
            & & & & e^{t\,\lambda_{n} }
    \end{pNiceMatrix}}\begin{pNiceMatrix}
        c_1\\ \Vdots \\ c_{n} 
    \end{pNiceMatrix} $

    In questo caso $ W(t) $ è anche di monodromia.
}{}{}
\paragrafo{Ritratto di fase per $\mathbf{ n=2} $}{%
    Siamo nel caso \[
        A=\begin{pmatrix}
            \lambda_1 & 0\\ 
            0 & \lambda_2
        \end{pmatrix}\,\leadsto\quad \begin{cases}
            x_1(t)= c_1\,e^{\lambda_1\,t}\\ 
            x_2(t)= c_2\,e^{\lambda_2\,t}
        \end{cases}
    \]\begin{itemize}
        \item Supponiamo che $ \lambda_1 \cdot \lambda_2\neq 0 $. L'equazione delle orbite è \[
            x_2=c\,x_1^{\lambda_2/\lambda_1}
        \]\begin{itemize}
            \item Se $ \lambda=\lambda_1=\lambda_2 $, allora le orbite sono di equazione $ \displaystyle x_2=c\,x_1 $: sono tutte rette. In particolare, tutte le rette passanti per l'origine sono orbite, \emph{anche gli assi}. 
            
            Per stabilire il verso di percorrenza delle orbite, si studia nuovamente il sistema  \[
                \begin{cases}
                    x_1(t)= c_1\,e^{\lambda\,t}\\ 
                    x_2(t)= c_2\,e^{\lambda\,t}
                \end{cases}
            \]si ha che \begin{itemize}
                \item se $\lambda>0$, $ \norma{\bm{x}(t)} \displaystyle \xrightarrow[t\to \infty]{}  + \infty  $, e quindi le semirette vengono percorse verso l'esterno;
                \item se $\lambda<0$, $ \norma{\bm{x}(t)} \xrightarrow[t\to \infty]{} 0$, e quindi le semirette vengono percorse verso l'interno.
            \end{itemize} 
            \item Consideriamo $ \lambda_1\neq \lambda_2 $, ma di segno concorde $ \leadsto \lambda_1 \cdot \lambda_2 > 0$. \begin{itemize}
                \item Se $ \lambda_2/\lambda_1>1 $ e sono entrambe positive, le orbite sono \emph{uscenti} e tangenti a $ x_1 $;
                \item Se $ \lambda_2/\lambda_1>1 $ e sono entrambe negative, le orbite sono \emph{entranti} e tangenti a $ x_1 $;
                \item se $ \lambda_2/\lambda_1<1 $ e sono entrambe positive, le orbite sono \emph{uscenti} e tangenti a $ x_2 $
                \item se $ \lambda_2/\lambda_1<1 $ e sono entrambe negative, le orbite sono \emph{entranti} e tangenti a $ x_2 $
            \end{itemize}
            \item Consideriamo $\lambda_1\neq \lambda_2$, ma di segno discorde $ \leadsto $ $\lambda_1 \cdot \lambda_2<0$. 
            
            In questo caso l'origine si chiama \emph{sella}, e si ha che \[
                x_2= c\,x_1^{\lambda_2/\lambda_1}.
            \]Essendo $ \lambda_2/\lambda_1<0 $, le orbite sono quelle di equazione $ y=c\,x^{\beta} $, eventualmente simmetrizzate rispetto all'asse delle $ y $.
        \end{itemize}
        \item Se $ \lambda_1 \cdot \lambda_2 = 0 $, suppongo che uno $\lambda_1 \neq 0$ (se fossero entrambi nulli, allora tutti i punti di $ \R^{2} $ sarebbero di equilibrio) \[
            \begin{cases}
                x_1'=\lambda\,x_1\\ 
                x_2'=0
            \end{cases}\,\leadsto\quad \begin{cases}
                x_1(t)=c_1\,e^{\lambda_1\,t}\\ 
                x_2\equiv c_2
            \end{cases}
        \]Dunque, quando $ c_1=0 $ ottengo infiniti punti di equilibrio, in quanto \[
            A=\begin{pmatrix}
                \lambda_1 & 0 \\ 
                 0 & 0
            \end{pmatrix}
        \]ha come nucleo tutto l'asse $ x_2 $. 

        Inoltre, si ha che se \begin{itemize}
            \item $\lambda_1>0$: tutti gli equilibri sono instabili;
            \item $\lambda_1<0$: tutti gli equilibri sono stabili, non asintotici.
        \end{itemize}
    \end{itemize}
}{}{}

\section{Matrice diagonalizzabile}
Adesso supponiamo che $A$ sia diagonalizzabile.

Sia $A\in \R^{n,n}$. Siano $\left\{\lambda_1,\dots,\lambda_n\right\}$ i suoi autovalori e $\left\{\bm{u}_1,\dots,\bm{u}_n\right\}$ i corrispondenti autovettori. Andiamo a dimostrare diversi fatti a riguardo:
\begin{itemize}
    \item $ce^{\lambda_i t}\bm{u}_i$ è una soluzione per ogni $c\in \R$ e $i=1,\dots,n$:
    \begin{align*}
        \frac{d}{dt}(ce^{\lambda_i t}\bm{u}_i)=c\lambda_ie^{\lambda_i t}\bm{u}_i\\
        A(ce^{\lambda_i t}\bm{u}_i)=ce^{\lambda_i t}A \bm{u}_i=ce^{\lambda_i t}\bm{u}_i \quad \qed
    \end{align*}
    \item $\varphi_i(t)=e^{\lambda_i t}\bm{u}_i, i=1,\dots, n$ sono una base di $S_0$: Questo perchè $\varphi_i(0)=\bm{u}_i$ sono linearmente indipendenti e per il lemma allora le funzioni proprio sono linearmente indipendenti
    \item La matrice wronskiana è:
    \begin{align*}
        W(t)=\begin{pNiceMatrix}
            \varphi_1(t) &\dots &\varphi_n (t)
        \end{pNiceMatrix}=\begin{pNiceMatrix}
            e^{\lambda_1 t}\bm{u}_1&\dots& e^{\lambda_n t}\bm{u}_n
        \end{pNiceMatrix}=\begin{pNiceMatrix}
            \bm{u}_1&\dots &\bm{u}_n
        \end{pNiceMatrix}=\\
        =\begin{pNiceMatrix}
    \bm{u}_1 & \dots &\bm{u}_n
        \end{pNiceMatrix}\begin{pNiceMatrix}
            e^{t\,\lambda_1}\\ 
                & e^{t\,\lambda_2}\\ 
                & & e^{t\,\lambda 3}\\ 
                & & & \Ddots\\ 
                & & & & e^{t\,\lambda_{n} }
        \end{pNiceMatrix}
    \end{align*}
    \item Poichè $W(0)=Q$, se poniamo:
    \begin{align*}
      \Phi(t)\coloneq W(t)Q^{-1} \leadsto \Phi(0)=Id
    \end{align*}
    e quindi $\Phi$ è di monodromia.
\end{itemize}
\paragrafo{Conclusione}{%
Quindi se $A$ è diagonalizzabile con $\left\{\lambda_1, \dots ,\lambda_n\right\}$ autovalori e $Q=(\bm{u}_1,\dots , \bm{u}_n)$ matrice con gli autovettori corrispondenti sulle colonne. Allora:
\begin{align*}
   S_0 = \left\{Q\begin{pmatrix}
        e^{t\,\lambda_1}\\ 
            & e^{t\,\lambda_2}\\ 
            & & e^{t\,\lambda_3}\\ 
            & & & \ddots\\ 
            & & & & e^{t\,\lambda_{n} }
    \end{pmatrix}Q^{-1}\bm{x}_0 \colon \bm{x}_0\in {\R}^n\right\}
\end{align*}
Ora ci chiediamo: come troviamo il ritratto di fase se A è diagonalizzabile?
}{}{}